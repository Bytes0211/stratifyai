In an AI prompt/response context, **temperature** is a setting that controls how *creative, random, or deterministic* the modelâ€™s output will be. Itâ€™s one of the most important knobs you can turn when shaping an LLMâ€™s behavior.

Hereâ€™s the simplest way to think about it:

# ğŸ”¥ Temperature = Creativity vs. Predictability

- **Low temperature (0.0â€“0.3)**  
  The model becomes *focused, precise, and deterministic*.  
  It picks the most likely next word almost every time.  
  Great for:  
  - factual answers  
  - coding  
  - math  
  - instructions  
  - summarization  

- **Medium temperature (0.4â€“0.7)**  
  Balanced, natural, conversational.  
  Good for:  
  - general chat  
  - brainstorming  
  - rewriting  
  - explanations  

- **High temperature (0.8â€“1.5)**  
  The model becomes *creative, surprising, and more random*.  
  It explores less likely word choices.  
  Great for:  
  - creative writing  
  - ideation  
  - fiction  
  - metaphors  
  - unusual or diverse outputs  

---

# ğŸ¯ Why Temperature Matters

Temperature affects **probability distribution**.  
A low temperature sharpens the distribution â†’ the model chooses the highestâ€‘probability token.  
A high temperature flattens the distribution â†’ the model samples from a wider range of possibilities.

In practice:

| Temperature | Behavior | Example |
|-------------|----------|---------|
| **0.0** | Deterministic | Always gives the same answer |
| **0.2** | Precise | â€œThe capital of France is Paris.â€ |
| **0.7** | Balanced | â€œParis is known for art, food, and culture.â€ |
| **1.2** | Creative | â€œParis feels like a living poem stitched together with light.â€ |

---

# ğŸ§  How it fits into your work (StratumAI)

Temperature becomes part of your **unified request schema**, letting you:

- Normalize behavior across providers  
- Tune creativity per model  
- Let the router choose temperature dynamically  
- Provide consistent defaults across OpenAI, Anthropic, Google, etc.  

Itâ€™s one of the few parameters that behaves consistently across all major LLM APIs.

---
